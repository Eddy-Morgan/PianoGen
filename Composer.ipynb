{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419ba25f-862d-4f02-aeec-b5564686ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw1 import Composer\n",
    "from midi2seq import process_midi_seq, seq2piano, random_piano, piano2seq, segment\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3596228-8fb9-4014-a8e3-e906c3734a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bcdb79e-6f0c-4c4b-9a0c-8d54911a2dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52111, 51)\n",
      "number of unique notes are 302 notes\n",
      "[ 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
      "  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56\n",
      "  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74\n",
      "  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92\n",
      "  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 149 150\n",
      " 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168\n",
      " 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186\n",
      " 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204\n",
      " 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222\n",
      " 223 224 225 226 227 228 229 230 231 232 233 234 235 236 256 257 258 259\n",
      " 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277\n",
      " 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295\n",
      " 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313\n",
      " 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331\n",
      " 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349\n",
      " 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378 379 380 381]\n"
     ]
    }
   ],
   "source": [
    "sequence = process_midi_seq(maxlen=50,n=50000)\n",
    "print(sequence.shape)\n",
    "\n",
    "notes = np.unique(sequence)\n",
    "print(f'number of unique notes are {len(notes)} notes')\n",
    "print(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb01bcab-e1b8-49f5-a5ac-6708342a06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "# sequence = scaler.fit_transform(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36ab97f0-5071-4995-8b40-8f61091d86fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52111, 50, 1]), torch.Size([52111, 1]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = sequence[:,:-1]\n",
    "X_train = X_train.reshape((-1,X_train.shape[1],1))\n",
    "\n",
    "Y_train = sequence[:,-1]\n",
    "Y_train = Y_train.reshape((-1,1))\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "Y_train = torch.tensor(Y_train).float()\n",
    "\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5000aa7-e495-4990-8668-f5b9ce904a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(21 == notes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "71fcd0eb-c3f6-46a6-97b5-b6387b500851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class MidiComposerDataset(Dataset):\n",
    "    def __init__(self,labels, x_sequence, y_next):\n",
    "        self.x_sequence = x_sequence\n",
    "        self.y_next = y_next\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_next)\n",
    "\n",
    "    def one_hot_encode(self, labels, note):\n",
    "        return torch.tensor(note == labels).float()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        action = int(self.y_next[idx][0])\n",
    "        encode_action = self.one_hot_encode(self.labels, action)\n",
    "        return dict(\n",
    "            sequence = self.x_sequence[idx],\n",
    "            action = encode_action\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1a752fcc-6dab-4a10-91e3-0db0d04894a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MidiComposerDataset(notes, X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d917da25-4a88-4dfc-9246-4e9d823f9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4e6b86fb-5599-4f2d-9b9c-041f94ed2bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 50, 1]) torch.Size([100, 302])\n"
     ]
    }
   ],
   "source": [
    "for _, batch in enumerate(train_loader):\n",
    "    sequence_batch , action_batch = batch['sequence'].to(device) , batch['action'].to(device) \n",
    "    print(sequence_batch.shape, action_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "839b5854-6867-4319-902e-6fee53279a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComposerModel(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, n_hidden=256, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.num_stacked_layers = n_layers\n",
    "        self.hidden_size = n_hidden\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = n_features,\n",
    "            hidden_size = n_hidden,\n",
    "            num_layers = n_layers,\n",
    "            batch_first = True,\n",
    "            dropout = 0.75\n",
    "        )\n",
    "        self.fc = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "40894f57-a9f8-46a7-85fa-4743aaf15f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComposerModel(\n",
       "  (lstm): LSTM(1, 128, num_layers=3, batch_first=True, dropout=0.75)\n",
       "  (fc): Linear(in_features=128, out_features=302, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ComposerModel(1, len(notes), 128, 3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d180d880-1e9f-42e5-bf61-7ebbaae6e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "30868d7e-101b-4f0e-9c62-5e0188b18cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    model.train(True)\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        sequence_batch , action_batch = batch['sequence'].to(device) , batch['action'].to(device)\n",
    "        \n",
    "        output = model(sequence_batch)\n",
    "        loss = loss_function(output, action_batch)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 100 == 99:  # print every 100 batches\n",
    "            avg_loss_across_batches = running_loss / 100\n",
    "            print('Batch {0}, Loss: {1:.3f}'.format(batch_index+1,\n",
    "                                                    avg_loss_across_batches))\n",
    "            running_loss = 0.0\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec84b27-cde4-4247-b169-eca61d39470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Batch 100, Loss: 4.359\n",
      "Batch 200, Loss: 4.337\n",
      "Batch 300, Loss: 4.346\n",
      "Batch 400, Loss: 4.349\n",
      "Batch 500, Loss: 4.362\n",
      "\n",
      "Epoch: 2\n",
      "Batch 100, Loss: 4.359\n",
      "Batch 200, Loss: 4.356\n",
      "Batch 300, Loss: 4.358\n",
      "Batch 400, Loss: 4.318\n",
      "Batch 500, Loss: 4.347\n",
      "\n",
      "Epoch: 3\n",
      "Batch 100, Loss: 4.338\n",
      "Batch 200, Loss: 4.346\n",
      "Batch 300, Loss: 4.332\n",
      "Batch 400, Loss: 4.330\n",
      "Batch 500, Loss: 4.351\n",
      "\n",
      "Epoch: 4\n",
      "Batch 100, Loss: 4.309\n",
      "Batch 200, Loss: 4.310\n",
      "Batch 300, Loss: 4.268\n",
      "Batch 400, Loss: 4.197\n",
      "Batch 500, Loss: 4.197\n",
      "\n",
      "Epoch: 5\n",
      "Batch 100, Loss: 4.149\n",
      "Batch 200, Loss: 4.111\n",
      "Batch 300, Loss: 4.106\n",
      "Batch 400, Loss: 4.090\n",
      "Batch 500, Loss: 4.059\n",
      "\n",
      "Epoch: 6\n",
      "Batch 100, Loss: 3.996\n",
      "Batch 200, Loss: 4.009\n",
      "Batch 300, Loss: 3.981\n",
      "Batch 400, Loss: 3.991\n",
      "Batch 500, Loss: 3.952\n",
      "\n",
      "Epoch: 7\n",
      "Batch 100, Loss: 3.941\n",
      "Batch 200, Loss: 3.905\n",
      "Batch 300, Loss: 3.940\n",
      "Batch 400, Loss: 3.848\n",
      "Batch 500, Loss: 3.865\n",
      "\n",
      "Epoch: 8\n",
      "Batch 100, Loss: 3.846\n",
      "Batch 200, Loss: 3.837\n",
      "Batch 300, Loss: 3.842\n",
      "Batch 400, Loss: 3.819\n",
      "Batch 500, Loss: 3.851\n",
      "\n",
      "Epoch: 9\n",
      "Batch 100, Loss: 3.811\n",
      "Batch 200, Loss: 3.835\n",
      "Batch 300, Loss: 3.795\n",
      "Batch 400, Loss: 3.779\n",
      "Batch 500, Loss: 3.747\n",
      "\n",
      "Epoch: 10\n",
      "Batch 100, Loss: 3.782\n",
      "Batch 200, Loss: 3.749\n",
      "Batch 300, Loss: 3.740\n",
      "Batch 400, Loss: 3.767\n",
      "Batch 500, Loss: 3.755\n",
      "\n",
      "Epoch: 11\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035834cf-6332-4554-9055-2323fc6d8724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
